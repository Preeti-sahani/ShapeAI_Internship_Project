{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+JLJn1CDAb5uMJ1enwpqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preeti-sahani/ShapeAI_Project/blob/main/Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxE18ID5Yorr"
      },
      "source": [
        "<h1 style=\"font-size:30px;color:black;text-align:center; text-decoration:underline\">                   An analysis of the most popular repos on Github</h1>\n",
        "<img src=\"https://kanbanize.com/blog/wp-content/uploads/2014/11/GitHub.jpg\" alt=\"github\" width=50% height=50%>\n",
        "<hr>\n",
        "\n",
        "# Scraping Top Repositories for Topics on GitHub\n",
        "Let's write a function to download the page.\n",
        "\n",
        "#Here are the steps we'll follow:\n",
        "*We're going to scrape https://github.com/topics\n",
        "*We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "*For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "*For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "*For each topic we'll create a CSV file in the following format:\n",
        "*Repo Name,Username,Stars,Repo URL\n",
        "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
        "\n",
        "\n",
        "*   We're going to scrape https://github.com/topics\n",
        "\n",
        "*   We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "\n",
        "\n",
        "\n",
        "*    For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "\n",
        "*    For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "\n",
        "\n",
        "*    For each topic we'll create a CSV file in the following format:\n",
        "\n",
        "*   Repo Name,Username,Stars,Repo URL\n",
        "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ajK9fAYYxJ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_topics_page():\n",
        "    # TODO - add comments\n",
        "    topic_url = 'https://github.com/topics'\n",
        "    response = requests.get(topic_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    # Parse using Beautiful soup\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return doc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNV2hEj4YYuS"
      },
      "source": [
        "doc = get_topics_page()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNdiwPpCYYq1"
      },
      "source": [
        "def get_topic_titles(doc):\n",
        "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
        "    topic_titles = []\n",
        "    for tag in topic_title_tags:\n",
        "        topic_titles.append(tag.text)\n",
        "    return topic_titles"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4hTkRhjcS6y"
      },
      "source": [
        "get_topic_titles can be used to get the list of titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXbb1-uYYoK",
        "outputId": "5e6dc98c-b6a0-4519-94d9-a6e7de3c068e"
      },
      "source": [
        "titles = get_topic_titles(doc)\n",
        "titles[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQG6MhWNZxAR"
      },
      "source": [
        "Similarly we have defined functions for descriptions and URLs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vaf5DWB4YYlc"
      },
      "source": [
        "def get_topic_descs(doc):\n",
        "    desc_selector = 'f5 color-text-secondary mb-0 mt-1'\n",
        "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
        "    topic_descs = []\n",
        "    for tag in topic_desc_tags:\n",
        "        topic_descs.append(tag.text.strip())\n",
        "    return topic_descs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ORW4YcZ4ML"
      },
      "source": [
        "TODO - example and explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US6o7AxyYYi6"
      },
      "source": [
        "def get_topic_urls(doc):\n",
        "    topic_link_tags = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
        "    topic_urls = []\n",
        "    base_url = 'https://github.com'\n",
        "    for tag in topic_link_tags:\n",
        "        topic_urls.append(base_url + tag['href'])\n",
        "    return topic_urls"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht6QOeoyZ99J"
      },
      "source": [
        "Let's put this all together into a single function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPjf7dLYYgW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def scrape_topics():\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    topics_dict = {\n",
        "        'title': get_topic_titles(doc),\n",
        "        'description': get_topic_descs(doc),\n",
        "        'url': get_topic_urls(doc)\n",
        "    }\n",
        "    return pd.DataFrame(topics_dict)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hor5quafcdsR"
      },
      "source": [
        "# Get the top 25 repositories from a topic page\n",
        "TODO - explanation and step\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbe7J4UWYYdp"
      },
      "source": [
        "def get_topic_page(topic_url):\n",
        "    # Download the page\n",
        "    response = requests.get(topic_url)\n",
        "    # Check successful response\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    # Parse using Beautiful soup\n",
        "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return topic_doc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08waTyOYYbD"
      },
      "source": [
        "doc = get_topic_page('https://github.com/topics/3d')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6SrQDo5cn55"
      },
      "source": [
        "TODO - talk about the h1 tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HCJWbe7clZy"
      },
      "source": [
        "def get_repo_info(h1_tag, star_tag):\n",
        "    # returns all the required info about a repository\n",
        "    a_tags = h1_tag.find_all('a')\n",
        "    username = a_tags[0].text.strip()\n",
        "    repo_name = a_tags[1].text.strip()\n",
        "    repo_url =  base_url + a_tags[1]['href']\n",
        "    stars = parse_star_count(star_tag.text.strip())\n",
        "    return username, repo_name, stars, repo_url\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhgGc2SBcw3x"
      },
      "source": [
        "TODO - show a example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otyZe_5ucugg"
      },
      "source": [
        "def get_topic_repos(topic_doc):\n",
        "    # Get the h1 tags containing repo title, repo URL and username\n",
        "    h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
        "    repo_tags = topic_doc.find_all('h3', {'class': h1_selection_class} )\n",
        "    # Get star tags\n",
        "    star_tags = topic_doc.find_all('a', { 'class': 'social-count float-none'})\n",
        "    \n",
        "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
        "\n",
        "    # Get repo info\n",
        "    for i in range(len(repo_tags)):\n",
        "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
        "        topic_repos_dict['username'].append(repo_info[0])\n",
        "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
        "        topic_repos_dict['stars'].append(repo_info[2])\n",
        "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
        "        \n",
        "    return pd.DataFrame(topic_repos_dict)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVw-puxOc2eI"
      },
      "source": [
        "TODO - show an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxTC1xIUc0sn"
      },
      "source": [
        "def scrape_topic(topic_url, path):\n",
        "    if os.path.exists(path):\n",
        "        print(\"The file {} already exists. Skipping...\".format(path))\n",
        "        return\n",
        "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "    topic_df.to_csv(path, index=None)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVAECaOoc9SC"
      },
      "source": [
        "TODO - show an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMZcMgtdBKa"
      },
      "source": [
        "#Putting it all together\n",
        "\n",
        "*   We have a funciton to get the list of topics\n",
        "*   We have a function to create a CSV file for scraped repos from a topics page\n",
        "*   Let's create a function to put them together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwuaA5rOc7Dq"
      },
      "source": [
        "import os\n",
        "def scrape_topics_repos():\n",
        "    print('Scraping list of topics')\n",
        "    topics_df = scrape_topics()\n",
        "    \n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    for index, row in topics_df.iterrows():\n",
        "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
        "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZwGUmWGdV8K"
      },
      "source": [
        "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "07ul_eP6dUJy",
        "outputId": "090bade1-d6b2-448f-d928-9419950c231b"
      },
      "source": [
        "scrape_topics_repos()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping list of topics\n",
            "Scraping top repositories for \"3D\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ac99c06d7fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_topics_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-68905d503c62>\u001b[0m in \u001b[0;36mscrape_topics_repos\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping top repositories for \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscrape_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-8cd33ef6c232>\u001b[0m in \u001b[0;36mscrape_topic\u001b[0;34m(topic_url, path)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The file {} already exists. Skipping...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtopic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_topic_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtopic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-571dae22cfa9>\u001b[0m in \u001b[0;36mget_topic_repos\u001b[0;34m(topic_doc)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get repo info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrepo_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_repo_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstar_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtopic_repos_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtopic_repos_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-607e3982c6a2>\u001b[0m in \u001b[0;36mget_repo_info\u001b[0;34m(h1_tag, star_tag)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrepo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrepo_url\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mstars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_star_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstar_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base_url' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTQZIlrNeOt5"
      },
      "source": [
        "We can check that the CSVs were created properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSZ3uXjBdYse"
      },
      "source": [
        "# read and display a CSV using Pandas\n",
        "\n",
        "#pd.read_csv('data/Android.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YV9zcpwjhql"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}